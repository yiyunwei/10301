{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#HW 7 - Programming (Transformers)\n",
        "\n",
        "Before you start on this programming assignment, make a copy of this file by clicking on the File button on the top left corner (right below the file name) and select \"Save a Copy in Drive\". Work on that copy and upload your completed transformer.ipynb file to Gradescope.\n",
        "\n",
        "Run **ALL** the cells in the notebook sequentially. Do **not** modify any other cells or the code may break.\n",
        "\n",
        "Cell with blanks to be filled in have \"TODOs\" and comments to explain what needs to be filled."
      ],
      "metadata": {
        "id": "TS5JwU6pTQ9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data import functional as textF\n",
        "from torch import nn, Tensor\n",
        "from typing import Tuple\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "\n",
        "import time\n",
        "from tqdm import trange, tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Currently available device is: \", device)\n",
        "\n",
        "print(\"Downloading the Wikitext dataset for pretraining\")\n",
        "url_wiki = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip'\n",
        "torchtext.utils.download_from_url(url_wiki)\n",
        "torchtext.utils.extract_archive('/content/.data/wikitext-103-v1.zip', './')"
      ],
      "metadata": {
        "id": "3qauobDu7aFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SentencePiece tokenizer on wikitext"
      ],
      "metadata": {
        "id": "1S52BKuMTM-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minutes to train tokenizer\n",
        "sp_model = textF.generate_sp_model('./wikitext-103/wiki.train.tokens', vocab_size = 50000, model_type='bpe', model_prefix = 'spm')\n",
        "sp_model = textF.load_sp_model(\"/content/spm.model\")"
      ],
      "metadata": {
        "id": "bl3-iska7i4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining on Wikitext"
      ],
      "metadata": {
        "id": "H6G9XNSmTQjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path, tokenizer_model):\n",
        "        self.tokenizer = textF.sentencepiece_tokenizer(tokenizer_model)\n",
        "        self.numericalizer = textF.sentencepiece_numericalizer(tokenizer_model)\n",
        "        self.train = self.numericalize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.numericalize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "\n",
        "    def flatten_extend(self, matrix):\n",
        "        flat_list = []\n",
        "        for row in matrix:\n",
        "            flat_list.extend(row)\n",
        "        return flat_list\n",
        "\n",
        "    def numericalize(self, path):\n",
        "        assert os.path.exists(path)\n",
        "        tok_lines = []\n",
        "        max_tok = 1000000\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            num_tok = 0\n",
        "            for i, line in tqdm(list(enumerate(f.readlines()))):\n",
        "                toks = list(self.numericalizer([line]))\n",
        "                toks = toks[0]\n",
        "                tok_lines.append(toks)\n",
        "                num_tok += len(toks)\n",
        "                if num_tok > max_tok:\n",
        "                    break\n",
        "        return torch.tensor(self.flatten_extend(tok_lines))\n",
        "\n",
        "def random_batch_sampler(tokens, device, batch_size, seq_len):\n",
        "    n_tokens = tokens.shape[0]\n",
        "    while True:\n",
        "        start_indices = torch.randint(0, n_tokens - seq_len + 1, (batch_size,))\n",
        "        sequences = torch.stack([tokens[start:start + seq_len] for start in start_indices])\n",
        "        yield sequences.to(device)\n",
        "\n",
        "\n",
        "def sequential_batch_sampler(tokens, device, batch_size, seq_len):\n",
        "    n_tokens = tokens.shape[0]\n",
        "    total_len = batch_size * seq_len\n",
        "\n",
        "    for i in range(0, n_tokens - total_len + 1, total_len):\n",
        "        batch = tokens[i:i + total_len].view(batch_size, seq_len)\n",
        "        yield batch.to(device)"
      ],
      "metadata": {
        "id": "v_L7Vfgp8A_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus(\"/content/wikitext-103\", sp_model)"
      ],
      "metadata": {
        "id": "Y_tpZnY0IZ0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seq_len = 65\n",
        "\n",
        "train_sampler = random_batch_sampler(corpus.train, device, batch_size, seq_len)\n",
        "val_sampler = sequential_batch_sampler(corpus.valid, device, batch_size, seq_len)"
      ],
      "metadata": {
        "id": "35jh1LJnkS2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition\n",
        "\n",
        "This cell contains the implementation for the Multihead attention for the transformer.\n",
        "\n",
        "Lines which need to be filled in are marked with **TODOs** with comments to explain the functionality to be implemented.\n",
        "\n",
        "**General hints**\n",
        "\n",
        "Make sure all of the data and the model weights (layers) are on the same device. If not use an appropriat method to ensure this.\n",
        "\n",
        "Make sure to use the appropriate dimensions while instantiating layers"
      ],
      "metadata": {
        "id": "qvg8eFhQ3iTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILL IN THIS BLOCK OF CODE\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "\n",
        "        '''\n",
        "        TODO: INSTANTIATE LINEAR LAYERS FOR CREATING QUERY, KEY AND VALUE VECTORS (Check suitable layer in torch.nn) FROM THE INPUT VECTOR.\n",
        "        Think about what the input and output dimensions must be for this layer.\n",
        "        '''\n",
        "\n",
        "        self.W_q = ... # Query transformation\n",
        "        self.W_k = ... # Key transformation\n",
        "        self.W_v = ... # Value transformation\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model).to(device) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "\n",
        "        '''\n",
        "        TODO: IMPLEMENT THE ATTENTION SCORE CALCULATION FROM THE QUERY AND KEY VECTORS\n",
        "        Make sure to multiply the Query vector with the transposed version of the Key vector, to calculate the attention score.\n",
        "        Use a suitable function from the torch library for this\n",
        "        '''\n",
        "\n",
        "        attn_scores = ...\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        '''\n",
        "        TODO: Multiply attn_probs by values to obtain the final output\n",
        "        Use a suitable function from the torch library for this\n",
        "        '''\n",
        "\n",
        "        output = ...\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "\n",
        "        '''\n",
        "        TODO: COMPUTE THE ATTENTION OUTPUT USING THE QUERY, KEY AND VALUE VECTORS USING THE SUITABLE HELPER FUNCTION FROM THIS CLASS\n",
        "        '''\n",
        "\n",
        "        attn_output = ...\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff).to(device)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model).to(device)\n",
        "        self.relu = nn.ReLU().to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        #use torch.nn.Embedding to create the encoding. Initialize dropout layer.\n",
        "        self.encoding = nn.Embedding(max_len, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, S, D = x.shape\n",
        "        positions = torch.arange(S).expand((N, -1)).to(x.device)\n",
        "        encoded_positions = self.encoding(positions)\n",
        "        output = x + encoded_positions\n",
        "        output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        '''\n",
        "        #TODO: INSTANTIATE A MULTIHEADATTENTION LAYER FOR THE GIVEN d_model AND num_heads\n",
        "        #Use the MultiHeadAttention class for this\n",
        "        '''\n",
        "\n",
        "        self.self_attn = ...\n",
        "\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "\n",
        "        '''\n",
        "        #TODO: IMPLEMENT THE LAYERNORM USED IN ATTENTION CALCULATION (Check suitable layer in torch.nn)\n",
        "        #This layer implements the layer normalization operation explained in the lecture\n",
        "        '''\n",
        "\n",
        "        self.norm1 = ...\n",
        "        self.norm2 = ...\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        '''\n",
        "        TODO: COMPUTE SELF ATTENTION ON THE GIVEN INPUT \"x\" AND \"mask\" USING THE \"self_attn\" layer\n",
        "        '''\n",
        "        attn_output = ...\n",
        "\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, device='cuda'):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, d_model).to(device)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size).to(device)\n",
        "        self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def generate_mask(self, src):\n",
        "        shape = src.shape[-1]\n",
        "        a = torch.ones(shape, shape)\n",
        "        mask = torch.tril(a).to(self.device)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src):\n",
        "        src_mask = self.generate_mask(src)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.embedding(src)))\n",
        "        enc_output = src_embedded\n",
        "        for layer in self.layers:\n",
        "            enc_output = layer(enc_output, src_mask)\n",
        "\n",
        "        return self.fc(enc_output)"
      ],
      "metadata": {
        "id": "lvWY8HiFlFvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pretraining (WikiText)"
      ],
      "metadata": {
        "id": "U_5QcoUoWHzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HYPERPARAMETERS FOR PRETRAINING\n",
        "\n",
        "src_vocab_size = 50000\n",
        "tgt_vocab_size = 50000\n",
        "d_model = 300\n",
        "num_heads = 2\n",
        "num_layers = 3\n",
        "d_ff = 300\n",
        "max_seq_length = 65\n",
        "dropout = 0.1\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "VkUVHCb_P9rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(data, logits):\n",
        "    labels = data[:,1:]\n",
        "    logits = logits[:,:-1]\n",
        "    labels = labels.reshape(-1)\n",
        "    logits = logits.reshape(-1, logits.size(-1))\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    return loss\n",
        "\n",
        "def train(model: nn.Module) -> float:\n",
        "    model.train()  # turn on train mode\n",
        "    iter_start_time = time.time()\n",
        "    total_loss = 0.\n",
        "    cnt = 0\n",
        "    iters_loss = 0.\n",
        "\n",
        "    for step in (pbar := trange(len(corpus.train) // batch_size)):\n",
        "        data = next(train_sampler)\n",
        "        output = model(data.to(device))\n",
        "\n",
        "        loss = compute_loss(data, output)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        iters_loss += loss.item()\n",
        "        if cnt % 2000 == 0 and cnt > 0:\n",
        "            train_loss = iters_loss / 2000\n",
        "            train_ppl = math.exp(train_loss)\n",
        "            elapsed = time.time() - iter_start_time\n",
        "            print('\\n' + '-' * 100)\n",
        "            print(f'| iteration {cnt:5d} | time elapsed : {elapsed:5.2f}s | '\n",
        "                f'train loss {train_loss:5.3f} | train perplexity {train_ppl:8.3f} | ')\n",
        "            print('-' * 100)\n",
        "            iters_loss = 0.\n",
        "        cnt += 1\n",
        "\n",
        "    return total_loss / cnt\n",
        "\n",
        "def evaluate(model: nn.Module) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    cnt = 0\n",
        "    val_sampler = sequential_batch_sampler(corpus.valid, device, batch_size, seq_len)\n",
        "    with torch.no_grad():\n",
        "        for data in (pbar := tqdm(val_sampler, desc=\"Evaluating..\")):\n",
        "            output = model(data.to(device))\n",
        "            loss = compute_loss(data, output)\n",
        "            total_loss += loss.item()\n",
        "            cnt += 1\n",
        "\n",
        "    return total_loss / cnt"
      ],
      "metadata": {
        "id": "5a8IL4-vQOoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# START TRAINING. The reference implementation takes around 1 hour to complete training.\n",
        "\n",
        "epoch_start_time = time.time()\n",
        "train_loss = train(model)\n",
        "train_ppl = math.exp(train_loss)\n",
        "val_loss = evaluate(model)\n",
        "val_ppl = math.exp(val_loss)\n",
        "elapsed = time.time() - epoch_start_time\n",
        "\n",
        "print('-' * 116)\n",
        "print(f'| end of epoch | time elasped: {elapsed:5.2f}s | '\n",
        "    f'train loss {train_loss:5.3f} | train perplexity {train_ppl:8.3f} | '\n",
        "    f'valid loss {val_loss:5.3f} | valid perplexity {val_ppl:8.3f} |')\n",
        "print('-' * 116)\n",
        "\n",
        "#SAVE THE PRETRAINED MODEL AS \"pretrained.pt\"\n",
        "\n",
        "print(\"saving the pretrained model\")\n",
        "torch.save(model.state_dict(), \"pretrained.pt\")"
      ],
      "metadata": {
        "id": "bBi97cETQQl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "9Hb0JQixnAIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning (Sentiment Analysis)\n"
      ],
      "metadata": {
        "id": "P_Kzhvs8Hs_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "\n",
        "documents = [(' '.join(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]"
      ],
      "metadata": {
        "id": "l4ZnsX9O8V1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(documents, num_samples):\n",
        "    i = num_samples // 2\n",
        "    neg_train = documents[:i]\n",
        "    pos_train = documents[-i:]\n",
        "    train = neg_train + pos_train\n",
        "    test = documents[i:-i]\n",
        "    random.shuffle(train)\n",
        "    random.shuffle(test)\n",
        "    return train, test\n",
        "\n",
        "sentiment_maxtokens = 64\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, input_data, tokenizer_model):\n",
        "      self.text = [t for (t, l) in input_data]\n",
        "      self.label = [1 if l == 'pos' else 0 for (t, l) in input_data]\n",
        "      self.tokenizer = textF.sentencepiece_tokenizer(tokenizer_model)\n",
        "      self.numericalizer = textF.sentencepiece_numericalizer(tokenizer_model)\n",
        "      self.numerictext = list(self.numericalizer(self.text))\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.label)\n",
        "\n",
        "  def get_sequence_token(self, idx):\n",
        "      sequence = self.numerictext[idx]\n",
        "      len_seq = len(sequence)\n",
        "      return sequence, len_seq\n",
        "\n",
        "  def get_labels(self, idx):\n",
        "      return self.label[idx]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      sequence, len_seq = self.get_sequence_token(idx)\n",
        "      label = self.get_labels(idx)\n",
        "      return sequence, label, len_seq\n",
        "\n",
        "def collate_fn(batch):\n",
        "    bs = len(batch)\n",
        "    sequences, labels, lengths = zip(*batch)\n",
        "    trunc_seqs = torch.zeros((bs, sentiment_maxtokens), dtype = torch.long)\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        if len(sequences[i]) < sentiment_maxtokens:\n",
        "            trunc_seqs[i][:len(sequences[i])] = torch.tensor(sequences[i], dtype = torch.long)\n",
        "        else:\n",
        "            trunc_seqs[i] = torch.tensor(torchtext.functional.truncate(sequences[i], sentiment_maxtokens), dtype = torch.long)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return trunc_seqs, labels"
      ],
      "metadata": {
        "id": "Kiv16MHY-u9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning without pretraining\n",
        "\n",
        "Train the model for the finetuning task for the different train dataset sizes for 20 epochs (The plot must be submitted as mentioned in the writeup)\n",
        "\n"
      ],
      "metadata": {
        "id": "eR8xKBAYuuZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HYPERPARAMETERS FOR FINETUNING TASK WITHOUT PRETRAINING\n",
        "\n",
        "src_vocab_size = 50000\n",
        "tgt_vocab_size = 50000\n",
        "d_model = 300\n",
        "num_heads = 2\n",
        "num_layers = 3\n",
        "d_ff = 300\n",
        "max_length = 256\n",
        "lr = 1e-4\n",
        "batch_size = 32\n",
        "finetune_epochs = 20\n",
        "\n",
        "#VARIOUS TRAINING SET SIZES FOR FINETUNING TASK\n",
        "'''\n",
        "#TODO: PLOT THE TRAINING ACCURACY vs NUM EPOCHS FOR DIFFERENT TRAINING SET SIZES\n",
        "'''\n",
        "sizes = [16, 32, 64, 128, 256, 512]"
      ],
      "metadata": {
        "id": "dxHGG_RyZeN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Tranformer architecture for the finetuning task without pretraining\n",
        "\n",
        "class NoPretrainingTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, d_ff, num_heads, num_layers, max_length, num_classes = 2, dropout = 0.1, device = 'cuda'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len = max_length)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        output = self.positional_encoding(embedded)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, None)\n",
        "        output = output[:, -1]\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "def train_classifier(model, dataset, sp_model, epochs, lr, bs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n",
        "    train_dataset = TextDataset(dataset, sp_model)\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=bs, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss_train = 0\n",
        "        total_acc_train = 0\n",
        "        num_pos = 0\n",
        "        for train_sequence, labels in tqdm(train_dataloader):\n",
        "            predictions = model(train_sequence.to(device))\n",
        "            labels = labels.to(device)\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            # Calculate accuracy and loss per batch\n",
        "            correct = predictions.argmax(axis=1) == labels\n",
        "            total_acc_train += correct.sum().item()\n",
        "            total_loss_train += loss.item()\n",
        "\n",
        "            # Backprop\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epochs: {epoch + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')\n",
        "\n",
        "def eval_classifier(model, dataset, sp_model, bs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n",
        "    test_dataset = TextDataset(dataset, sp_model)\n",
        "    test_dataloader = DataLoader(test_dataset, num_workers=1, batch_size=bs, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    total_acc_train = 0.\n",
        "    for sequence, labels in tqdm(test_dataloader):\n",
        "        predictions = model(sequence.to(device))\n",
        "        labels = labels.to(device)\n",
        "        correct = predictions.argmax(axis=1) == labels\n",
        "        total_acc_train += correct.sum().item()\n",
        "\n",
        "    return total_acc_train / len(test_dataset)"
      ],
      "metadata": {
        "id": "IG3ALVyWVOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train loop\n",
        "train_sets = {}\n",
        "models = {}\n",
        "for sz in sizes:\n",
        "    train_sets[sz], test_set = train_test_split(documents, sz)\n",
        "    models[sz] = NoPretrainingTransformer(src_vocab_size, d_model, d_ff, num_heads, num_layers, max_length)\n",
        "    print('-' * 56)\n",
        "    print(f'Training on {sz} samples')\n",
        "    print('-' * 56)\n",
        "    train_classifier(models[sz], train_sets[sz], sp_model, finetune_epochs, lr, batch_size)\n",
        "\n",
        "#Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for sz in models:\n",
        "        print('-' * 56)\n",
        "        print(f'Evaluating model trained on {sz} samples')\n",
        "        print('-' * 56)\n",
        "        test_accuracy = eval_classifier(models[sz], test_set, sp_model, batch_size)\n",
        "        print(\"\\n Accuracy on test set = %.3f\" % test_accuracy)"
      ],
      "metadata": {
        "id": "RkJ5fHMmvKO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finetuning with pretraining\n",
        "\n",
        "Train the model for the finetuning task for the different train dataset sizes for 20 epochs (The plot must be submitted as mentioned in the writeup)\n"
      ],
      "metadata": {
        "id": "MMZr9IJ0H77a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, dataset, sp_model, epochs, lr, bs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n",
        "    train_dataset = TextDataset(dataset, sp_model)\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=bs, collate_fn=collate_fn, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss_train = 0\n",
        "        total_acc_train = 0\n",
        "        num_pos = 0\n",
        "        for train_sequence, labels in tqdm(train_dataloader):\n",
        "            predictions = model(train_sequence.to(device))\n",
        "            preds = predictions[:, -1]\n",
        "            labels = labels.to(device)\n",
        "            loss = criterion(preds, labels)\n",
        "\n",
        "            # Calculate accuracy and loss per batch\n",
        "            correct = preds.argmax(axis=1) == labels\n",
        "            total_acc_train += correct.sum().item()\n",
        "            total_loss_train += loss.item()\n",
        "\n",
        "            # Backprop\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epochs: {epoch + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')"
      ],
      "metadata": {
        "id": "HGV0Jzr_6Z2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "batch_size = 64\n",
        "finetune_epochs = 20\n",
        "\n",
        "#Train loop\n",
        "train_sets = {}\n",
        "finetuned_models = {}\n",
        "for sz in sizes:\n",
        "    train_sets[sz], test_set = train_test_split(documents, sz)\n",
        "    pretrained = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "    pretrained.load_state_dict(torch.load('/content/pretrained.pt'))\n",
        "    finetuned_models[sz] = pretrained\n",
        "    finetuned_models[sz].fc = nn.Linear(d_model, 2).to(device)\n",
        "    print('-' * 56)\n",
        "    print(f'Training on {2 * sz} samples')\n",
        "    print('-' * 56)\n",
        "    train_classifier(finetuned_models[sz], train_sets[sz], sp_model, finetune_epochs, lr, batch_size)"
      ],
      "metadata": {
        "id": "3E89aiVnL1HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_classifier(model, dataset, sp_model, bs):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n",
        "    test_dataset = TextDataset(dataset, sp_model)\n",
        "    test_dataloader = DataLoader(test_dataset, num_workers=1, batch_size=bs, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    total_acc = 0.\n",
        "    for sequence, labels in tqdm(test_dataloader):\n",
        "        predictions = model(sequence.to(device))\n",
        "        preds = predictions[:, -1]\n",
        "        labels = labels.to(device)\n",
        "        correct = preds.argmax(axis=1) == labels\n",
        "        total_acc += correct.sum().item()\n",
        "\n",
        "    return total_acc / len(test_dataset)"
      ],
      "metadata": {
        "id": "V2rWFXFG4QzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for sz in finetuned_models:\n",
        "        print('-' * 56)\n",
        "        print(f'Evaluating model trained on {sz} samples')\n",
        "        print('-' * 56)\n",
        "        test_accuracy = eval_classifier(finetuned_models[sz], test_set, sp_model, batch_size)\n",
        "        print(\"\\n Accuracy on test set = %.3f\" % test_accuracy)"
      ],
      "metadata": {
        "id": "OlrpclAS6Vwi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}